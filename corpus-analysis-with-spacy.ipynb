{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/programminghistorian/jekyll/blob/Issue-3052/assets/corpus-analysis-with-spacy/corpus-analysis-with-spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVeD4Ik7D43F"
   },
   "source": [
    "# Corpus Analysis with spaCy\n",
    "## by Megan S. Kane\n",
    "#### https://programminghistorian.org/en/lessons/corpus-analysis-with-spacy\n",
    "(Slightly adapted by Yevgen Matusevych for Collecting Data class at RUG.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Say you have a big collection of texts. Maybe you’ve gathered speeches from the French Revolution, compiled a bunch of Amazon product reviews, or unearthed a collection of diary entries written during the first world war. In any of these cases, computational analysis can be a good way to compliment close reading of your corpus… but where should you start?\n",
    "\n",
    "One possible way to begin is with spaCy, an industrial-strength library for Natural Language Processing (NLP) in Python. spaCy is capable of processing large corpora, generating linguistic annotations including part-of-speech tags and named entities, as well as preparing texts for further machine classification. This lab is a ‘spaCy 101’ of sorts, a primer for researchers who are new to spaCy and want to learn how it can be used for corpus analysis. It may also be useful for those who are curious about natural language processing tools in general, and how they can help us to answer humanities research questions.\n",
    "\n",
    "\n",
    "### Lab Goals\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Upload a corpus of texts to a platform for Python analysis (using Google Colaboratory)\n",
    "- Use spaCy to enrich the corpus through tokenization, lemmatization, part-of-speech tagging and named entity recognition\n",
    "- Conduct frequency analyses using part-of-speech tags and named entities\n",
    "- Download an enriched dataset for use in future NLP analyses\n",
    "\n",
    "\n",
    "\n",
    "### Why Use spaCy for Corpus Analysis?\n",
    "\n",
    "As the name implies, corpus analysis involves studying corpora, or large collections of documents. Typically, the documents in a corpus are representative of the group(s) a researcher is interested in studying, such as the writings of a specific author or genre. By analyzing these texts at scale, researchers can identify meaningful trends in the way language is used within the target group(s).\n",
    "\n",
    "Though computational tools like spaCy can’t read and comprehend the meaning of texts like humans do, they excel at ‘parsing’ (analyzing sentence structure) and ‘tagging’ (labeling) them. When researchers give spaCy a corpus, it will ‘parse’ every document in the collection, identifying the grammatical categories to which each word and phrase in each text most likely belongs. NLP Algorithms like spaCy use this information to generate lexico-grammatical tags that are of interest to researchers, such as lemmas (base words), part-of-speech tags and named entities (more on these in the Part-of-Speech Analysis and Named Entity Recognition sections below). Furthermore, computational tools like spaCy can perform these parsing and tagging processes much more quickly (in a matter of seconds or minutes) and on much larger corpora (hundreds, thousands, or even millions of texts) than human readers would be able to.\n",
    "\n",
    "Though spaCy was designed for industrial use in software development, researchers also find it valuable for several reasons:\n",
    "\n",
    "- It’s easy to set up and use spaCy’s Trained Models and Pipelines; there is no need to call a wide range of packages and functions for each individual task\n",
    "- It uses fast and accurate algorithms for text-processing tasks, which are kept up-to-date by the developers so it’s efficient to run\n",
    "- It performs better on text-splitting tasks than Natural Language Toolkit (NLTK), because it constructs syntactic trees for each sentence\n",
    "- You may still be wondering: What is the value of extracting language data such as lemmas, part-of-speech tags, and named entities from a corpus? How can this data help researchers answer meaningful humanities research questions? To illustrate, let’s look at the example corpus and questions developed for this lab.\n",
    "\n",
    "\n",
    "\n",
    "### Dataset: Michigan Corpus of Upper-Level Student Papersdataset-michigan-corpus-of-upper-level-student-papers\n",
    "\n",
    "The Michigan Corpus of Upper-Level Student Papers (MICUSP) is a corpus of 829 high-scoring academic writing samples from students at the University of Michigan. The texts come from 16 disciplines and seven genres, all were written by senior undergraduate or graduate students and received an A-range score in a university course.1 The texts and their metadata are publicly available on MICUSP Simple, an online interface which allows users to search for texts by a range of fields (for example genre, discipline, student level, textual features) and conduct simple keyword analyses across disciplines and genres.\n",
    "\n",
    "This lab will explore a subset of documents from MICUSP: 67 Biology papers and 98 English papers. Writing samples in this select corpus belong to all seven MICUSP genres: Argumentative Essay, Creative Writing, Critique/Evaluation, Proposal, Report, Research Paper, and Response Paper. This select corpus .txt_files.zip and the associated metadata.csv are available to download as sample materials for this lab. The dataset has been culled from the larger corpus in order to investigate the differences between two distinct disciplines of academic writing (Biology and English). It is also a manageable size for the purposes of this lab.\n",
    "\n",
    "\n",
    "### Research Questions: Linguistic Differences Within Student Paper Genres and Disciplines\n",
    "This lab will describe how spaCy’s utilities in stopword removal, tokenization, and lemmatization can assist in (and hinder) the preparation of student texts for analysis. You will learn how spaCy’s ability to extract linguistic annotations such as part-of-speech tags and named entities can be used to compare conventions within subsets of a discursive community of interest. The lab focuses on lexico-grammatical features that may indicate genre and disciplinary differences in academic writing.\n",
    "\n",
    "The following research questions will be investigated:\n",
    "\n",
    "1: Do students use certain parts-of-speech more frequently in Biology texts versus English texts, and does this linguistic discrepancy signify differences in disciplinary conventions?\n",
    "Prior research has shown that even when writing in the same genres, writers in the sciences follow different conventions than those in the humanities. Notably, academic writing in the sciences has been characterized as informational, descriptive, and procedural, while scholarly writing in the humanities is narrativized, evaluative, and situation-dependent (that is, focused on discussing a particular text or prompt)5. By deploying spaCy on the MICUSP texts, researchers can determine whether there are any significant differences between the part-of-speech tag frequencies in English and Biology texts. For example, we might expect students writing Biology texts to use more adjectives than those in the humanities, given their focus on description. Conversely, we might suspect English texts to contain more verbs and verb auxiliaries, indicating a more narrative structure. To test these hypotheses, you’ll learn to analyze part-of-speech counts generated by spaCy, as well as to explore other part-of-speech count differences that could prompt further investigation.\n",
    "\n",
    "2: Do students use certain named entities more frequently in different academic genres, and do these varying word frequencies signify broader differences in genre conventions?\n",
    "As with disciplinary differences, research has shown that different genres of writing have their own conventions and expectations. For example, explanatory genres such as research papers, proposals and reports tend to focus on description and explanation, whereas argumentative and critique-driven texts are driven by evaluations and arguments6. By deploying spaCy on the MICUSP texts, researchers can determine whether there are any significant differences between the named entity frequencies in texts within the seven different genres represented (Argumentative Essay, Creative Writing, Critique/Evaluation, Proposal, Report, Research Paper, and Response Paper). We may suspect that argumentative genres engage more with people or works of art, since these could be entities serving to support their arguments or as the subject of their critiques. Conversely, perhaps dates and numbers are more prevalent in evidence-heavy genres, such as research papers and proposals. To test these hypotheses, you’ll learn to analyze the nouns and noun phrases spaCy has tagged as ‘named entities.’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ8ve667EvxG"
   },
   "source": [
    "### Installing, Importing and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Install English language model\n",
    "!spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlvUa2oX1645"
   },
   "outputs": [],
   "source": [
    "# Import os to upload documents and metadata\n",
    "import os\n",
    "\n",
    "# Load spaCy visualizer\n",
    "from spacy import displacy\n",
    "\n",
    "# Import pandas DataFrame packages\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# Import graphing package\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ub8MflGfA_HB"
   },
   "outputs": [],
   "source": [
    "# Create empty lists for file names and contents\n",
    "texts = []\n",
    "file_names = []\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for _file_name in os.listdir('txt_files'):\n",
    "# Look for only text files\n",
    "    if _file_name.endswith('.txt'):\n",
    "    # Append contents of each text file to text list\n",
    "        texts.append(open('txt_files' + '/' + _file_name, 'r', encoding='utf-8').read())\n",
    "        # Append name of each file to file name list\n",
    "        file_names.append(_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7BmHGFBA_HB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary object associating each file name with its text\n",
    "d = {'Filename':file_names,'Text':texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yC5-sbOPA_HB"
   },
   "outputs": [],
   "source": [
    "# Turn dictionary into a dataframe\n",
    "paper_df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cK3PvJkcA_HC"
   },
   "outputs": [],
   "source": [
    "paper_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beginnings of some of the texts may contain extra spaces (indicated by \\t or \\n). These characters can be replaced by a single space using the str.replace() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kax8Ecu7A_HC"
   },
   "outputs": [],
   "source": [
    "# Remove extra spaces from papers\n",
    "paper_df['Text'] = paper_df['Text'].str.replace('\\s+', ' ', regex=True).str.strip()\n",
    "paper_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGbZVIrFA_HC"
   },
   "outputs": [],
   "source": [
    "# Load metadata.\n",
    "metadata_df = pd.read_csv('metadata.csv')\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RO4lwuwJcrID"
   },
   "outputs": [],
   "source": [
    "# Remove .txt from title of each paper\n",
    "paper_df['Filename'] = paper_df['Filename'].str.replace('.txt', '', regex=True)\n",
    "\n",
    "# Rename column from paper ID to Title\n",
    "metadata_df.rename(columns={\"PAPER ID\": \"Filename\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eCYbDExeuqM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge metadata and papers into new DataFrame\n",
    "# Will only keep rows where both essay and metadata are present\n",
    "final_paper_df = metadata_df.merge(paper_df,on='Filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the head of the DataFrame again to confirm everything has worked well. Check the first five rows to make sure each has a filename, title, discipline, paper type and text (the full paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-8mOPIZA_HD"
   },
   "outputs": [],
   "source": [
    "# Print DataFrame\n",
    "final_paper_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting DataFrame is now ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdlHoqlBA_HD"
   },
   "source": [
    "## Text Enrichment with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejyC6xvA3w9q"
   },
   "source": [
    "### Creating Doc Objects\n",
    "\n",
    "\n",
    "To use spaCy, the first step is to load one of spaCy’s Trained Models and Pipelines which will be used to perform tokenization, part-of-speech tagging, and other text enrichment tasks. A wide range of options are available (see the full list here), and they vary based on size and language.\n",
    "\n",
    "We’ll use en_core_web_sm, which has been trained on written web texts. It may not perform as accurately as the those trained on medium and large English language models, but it will deliver results most efficiently. Once we’ve loaded en_core_web_sm, we can check what actions it performs; parser, tagger, lemmatizer, and NER, should be among those listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2P1TUR3oA_HD"
   },
   "outputs": [],
   "source": [
    "# Load nlp pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Check what functions it performs\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the nlp function is loaded, let’s test out its capacities on a single sentence. Calling the nlp function on a single sentence yields a Doc object. This object stores not only the original text, but also all of the linguistic annotations obtained when spaCy processed the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ceknDmJZA_HE"
   },
   "outputs": [],
   "source": [
    "#Define example sentence\n",
    "sentence = \"This is 'an' example? sentence\"\n",
    "\n",
    "# Call the nlp model on the sentence\n",
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can call on the Doc object to get the information we’re interested in. The command below loops through each token in a Doc object and prints each word in the text along with its corresponding part-of-speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWX1hxoBA_HE"
   },
   "outputs": [],
   "source": [
    "# Loop through each token in doc object\n",
    "for token in doc:\n",
    "    # Print text and part of speech for each\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try the same process on the student texts. As we’ll be calling the NLP function on every text in the DataFrame, we should first define a function that runs nlp on whatever input text is given. Functions are a useful way to store operations that will be run multiple times, reducing duplications and improving code readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pE_8YJ_A_HE"
   },
   "outputs": [],
   "source": [
    "# Define a function that runs the nlp pipeline on any given input text\n",
    "def process_text(text):\n",
    "    return nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the function is defined, use .apply() to apply it to every cell in a given DataFrame column. In this case, nlp will run on each cell in the Text column of the final_paper_df DataFrame, creating a Doc object from every student text. These Doc objects will be stored in a new column of the DataFrame called Doc.\n",
    "\n",
    "Running this function takes several minutes because spaCy is performing all the parsing and tagging tasks on each text. However, when it is complete, we can simply call on the resulting Doc objects to get parts-of-speech, named entities, and other information of interest, just as in the example of the sentence above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3O8qhn1A_HE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the function to the \"Text\" column, so that the nlp pipeline is called on each student essay\n",
    "final_paper_df['Doc'] = final_paper_df['Text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSNTL5msA_HE"
   },
   "source": [
    "### Text Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tgNt7NO35I0"
   },
   "source": [
    "#### Tokenization\n",
    "\n",
    "A critical first step spaCy performs is tokenization, or the segmentation of strings into individual words and punctuation markers. Tokenization enables spaCy to parse the grammatical structures of a text and identify characteristics of each word-like part-of-speech.\n",
    "\n",
    "To retrieve a tokenized version of each text in the DataFrame, we’ll write a function that iterates through any given Doc object and returns all functions found within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBEb3CErA_HE"
   },
   "outputs": [],
   "source": [
    "# Define a function to retrieve tokens from a doc object\n",
    "def get_token(doc):\n",
    "    return [(token.text) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the function used to create Doc objects, the token function can be applied to the DataFrame. In this case, we will call the function on the Doc column, since this is the column which stores the results from the processing done by spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-qpJ-peA_HF"
   },
   "outputs": [],
   "source": [
    "# Run the token retrieval function on the doc objects in the dataframe\n",
    "final_paper_df['Tokens'] = final_paper_df['Doc'].apply(get_token)\n",
    "final_paper_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the Text and Tokens column, we find a couple of differences. In the table below, you’ll notice that most importantly, the words, spaces, and punctuation markers in the Tokens column are separated by commas, indicating that each have been parsed as individual tokens. The text in the Tokens column is also bracketed; this indicates that tokens have been generated as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSU8Rn57FbSK"
   },
   "outputs": [],
   "source": [
    "tokens = final_paper_df[['Text', 'Tokens']].copy()\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLzPDKXqA_HF"
   },
   "source": [
    "#### Lemmatization\n",
    "\n",
    "Another process performed by spaCy is lemmatization, or the retrieval of the dictionary root word of each word (for example “brighten” for “brightening”). We’ll perform a similar set of steps to those above to create a function to call the lemmas from the Doc object, then apply it to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50hlrH_KA_HF"
   },
   "outputs": [],
   "source": [
    "# Define a function to retrieve lemmas from a doc object\n",
    "def get_lemma(doc):\n",
    "    return [(token.lemma_) for token in doc]\n",
    "\n",
    "# Run the lemma retrieval function on the doc objects in the dataframe\n",
    "final_paper_df['Lemmas'] = final_paper_df['Doc'].apply(get_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization can help reduce noise and refine results for researchers who are conducting keyword searches. For example, let’s compare counts of the word “write” in the original Tokens column and in the lemmatized Lemmas column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbrgpAIjA_HF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'\"Write\" appears in the text tokens column ' + str(final_paper_df['Tokens'].apply(lambda x: x.count('write')).sum()) + ' times.')\n",
    "print(f'\"Write\" appears in the lemmas column ' + str(final_paper_df['Lemmas'].apply(lambda x: x.count('write')).sum()) + ' times.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there are more instances of “write” in the Lemmas column, as the lemmatization process has grouped inflected word forms (writing, writer) into the base word “write.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvH1xTvZ3-MW"
   },
   "source": [
    "### Text Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLeLfyvVA_HF"
   },
   "source": [
    "#### Part of Speech Tagging\n",
    "\n",
    "spaCy facilitates two levels of part-of-speech tagging: coarse-grained tagging, which predicts the simple universal part-of-speech of each token in a text (such as noun, verb, adjective, adverb), and detailed tagging, which uses a larger, more fine-grained set of part-of-speech tags (for example 3rd person singular present verb). The part-of-speech tags used are determined by the English language model we use. In this case, we’re using the small English model, and you can explore the differences between the models on spaCy’s website.\n",
    "\n",
    "We can call the part-of-speech tags in the same way as the lemmas. Create a function to extract them from any given Doc object and apply the function to each Doc object in the DataFrame. The function we’ll create will extract both the coarse- and fine-grained part-of-speech for each token (token.pos_ and token.tag_, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T24RkcuTA_HF"
   },
   "outputs": [],
   "source": [
    "# Define a function to retrieve lemmas from a doc object\n",
    "def get_pos(doc):\n",
    "    #Return the coarse- and fine-grained part of speech text for each token in the doc\n",
    "    return [(token.pos_, token.tag_) for token in doc]\n",
    "\n",
    "# Define a function to retrieve parts of speech from a doc object\n",
    "final_paper_df['POS'] = final_paper_df['Doc'].apply(get_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a list of the part-of-speech columns to review them further. The first (coarse-grained) tag corresponds to a generally recognizable part-of-speech such as a noun, adjective, or punctuation mark, while the second (fine-grained) category are a bit more difficult to decipher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1fZXUynA_HF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list of part of speech tags\n",
    "list(final_paper_df['POS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, spaCy has a built-in function called explain that can provide a short description of any tag of interest. If we try it on the tag IN using spacy.explain(\"IN\"), the output reads conjunction, subordinating or preposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lhKj9xGA_HG"
   },
   "outputs": [],
   "source": [
    "spacy.explain(\"IN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, you may want to get only a set of part-of-speech tags for further analysis, like all of the proper nouns. A function can be written to perform this task, extracting only words which have been fitted with the proper noun tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-k8yoClA_HG"
   },
   "outputs": [],
   "source": [
    "# Define function to extract proper nouns from Doc object\n",
    "def extract_proper_nouns(doc):\n",
    "    return [token.text for token in doc if token.pos_ == 'PROPN']\n",
    "\n",
    "# Apply function to Doc column and store resulting proper nouns in new column\n",
    "final_paper_df['Proper_Nouns'] = final_paper_df['Doc'].apply(extract_proper_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing the nouns in each text can help us ascertain the texts’ subjects. Let’s list the nouns in two different texts, the text located in row 3 of the DataFrame and the text located in row 163."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m98pVVJX1ZlK"
   },
   "outputs": [],
   "source": [
    "list(final_paper_df.loc[[3, 163], 'Proper_Nouns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first text in the list includes botany and astronomy concepts; this is likely to have been written for a biology course. In contrast, the second text appears to be an analysis of Shakespeare plays and movie adaptations, likely written for an English course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39utqaRyh_M1"
   },
   "source": [
    "#### Named Entity Recognition\n",
    "\n",
    "spaCy can tag named entities in the text, such as names, dates, organizations, and locations. Call the full list of named entities and their descriptions using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRffhMlUA_HI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get all NE labels and assign to variable\n",
    "labels = nlp.get_pipe(\"ner\").labels\n",
    "\n",
    "# Print each label and its description\n",
    "for label in labels:\n",
    "    print(label + ' : ' + spacy.explain(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll create a function to extract the named entity tags from each Doc object and apply it to the Doc objects in the DataFrame, storing the named entities in a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mow27zbdA_HI"
   },
   "outputs": [],
   "source": [
    "# Define function to extract named entities from doc objects\n",
    "def extract_named_entities(doc):\n",
    "    return [ent.label_ for ent in doc.ents]\n",
    "\n",
    "# Apply function to Doc column and store resulting named entities in new column\n",
    "final_paper_df['Named_Entities'] = final_paper_df['Doc'].apply(extract_named_entities)\n",
    "final_paper_df['Named_Entities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add another column with the words and phrases identified as named entities:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQYCg6fkA_HJ"
   },
   "outputs": [],
   "source": [
    "# Define function to extract text tagged with named entities from doc objects\n",
    "def extract_named_entities(doc):\n",
    "    return [ent for ent in doc.ents]\n",
    "\n",
    "# Apply function to Doc column and store resulting text in new column\n",
    "final_paper_df['NE_Words'] = final_paper_df['Doc'].apply(extract_named_entities)\n",
    "final_paper_df['NE_Words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s visualize the words and their named entity tags in a single text. Call the first text’s Doc object and use displacy.render to visualize the text with the named entities highlighted and tagged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ovPPNn4A_HJ"
   },
   "outputs": [],
   "source": [
    "# Extract the first Doc object\n",
    "doc = final_paper_df['Doc'][1]\n",
    "\n",
    "# Visualize named entity tagging in a single paper\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’d like to review the output of this code as raw .html, you can download it here:\n",
    "https://programminghistorian.org/assets/corpus-analysis-with-spacy/corpus-analysis-with-spacy-20.html\n",
    "    \n",
    "Named entity recognition enables researchers to take a closer look at the ‘real-world objects’ that are present in their texts. The rendering allows for close-reading of these entities in context, their distinctions helpfully color-coded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKiNOMoDB2ta"
   },
   "source": [
    "### Download Enriched Dataset\n",
    "\n",
    "To save the dataset of doc objects, text reductions and linguistic annotations generated with spaCy, download the final_paper_df DataFrame to your local computer as a .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7USU7ipB9YK"
   },
   "outputs": [],
   "source": [
    "# Save DataFrame as csv (in Google Drive)\n",
    "# Use this step only to save  csv to your computer's working directory\n",
    "final_paper_df.to_csv('MICUSP_papers_with_spaCy_tags.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUGnN4BX4C4c"
   },
   "source": [
    "## Analysis of Linguistic Annotations\n",
    "\n",
    "Why are spaCy’s linguistic annotations useful to researchers? Below are two examples of how researchers can use data about the MICUSP corpus, produced through spaCy, to draw conclusions about discipline and genre conventions in student academic writing. We will use the enriched dataset generated with spaCy for these examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QTJrt6byTlf"
   },
   "source": [
    "### Part of Speech Analysis\n",
    "\n",
    "In this section, we’ll analyze the part-of-speech tags extracted by spaCy to answer the first research question: Do students use certain parts-of-speech more frequently in Biology texts versus English texts, and does this signify differences in disciplinary conventions?\n",
    "\n",
    "spaCy counts the number of each part-of-speech tag that appears in each document (for example the number of times the NOUN tag appears in a document). This is called using doc.count_by(spacy.attrs.POS). Here’s how it works on a single sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5RAD_t_SJt2"
   },
   "outputs": [],
   "source": [
    "# Create doc object from single sentence\n",
    "doc = nlp(\"This is 'an' example? sentence\")\n",
    "\n",
    "# Print counts of each part of speech in sentence\n",
    "print(doc.count_by(spacy.attrs.POS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy generates a dictionary where the values represent the counts of each part-of-speech term found in the text. The keys in the dictionary correspond to numerical indices associated with each part-of-speech tag. To make the dictionary more legible, let’s associate the numerical index values with their corresponding part of speech tags. In the example below, it’s now possible to see which parts-of-speech tags correspond to which counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGPdo6FtS4bq"
   },
   "outputs": [],
   "source": [
    "# Store dictionary with indexes and POS counts in a variable\n",
    "num_pos = doc.count_by(spacy.attrs.POS)\n",
    "\n",
    "dictionary = {}\n",
    "\n",
    "# Create a new dictionary which replaces the index of each part of speech for its label (NOUN, VERB, ADJECTIVE)\n",
    "for k,v in sorted(num_pos.items()):\n",
    "    dictionary[doc.vocab[k].text] = v\n",
    "\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the same type of dictionary for each text in a DataFrame, a function can be created to nest the above for loop. First, we’ll create a new DataFrame for the purposes of part-of speech analysis, containing the text filenames, disciplines, and Doc objects. We can then apply the function to each Doc object in the new DataFrame. In this case (and above), we are interested in the simpler, coarse-grained parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TnDn9MpTA_HK"
   },
   "outputs": [],
   "source": [
    "# Create new DataFrame for analysis purposes\n",
    "pos_analysis_df = final_paper_df[['Filename','DISCIPLINE', 'Doc']]\n",
    "\n",
    "# Create list to store each dictionary\n",
    "num_list = []\n",
    "\n",
    "# Define a function to get part of speech tags and counts and append them to a new dictionary\n",
    "def get_pos_tags(doc):\n",
    "    dictionary = {}\n",
    "    num_pos = doc.count_by(spacy.attrs.POS)\n",
    "    for k,v in sorted(num_pos.items()):\n",
    "        dictionary[doc.vocab[k].text] = v\n",
    "    num_list.append(dictionary)\n",
    "\n",
    "# Apply function to each doc object in DataFrame\n",
    "pos_analysis_df.loc['C_POS'] = pos_analysis_df['Doc'].apply(get_pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we’ll take the part-of-speech counts and put them into a new DataFrame where we can calculate the frequency of each part-of-speech per document. In the new DataFrame, if a paper does not contain a particular part-of-speech, the cell will read NaN (Not a Number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQ0UCcqMA_HK"
   },
   "outputs": [],
   "source": [
    "# Create new dataframe with part of speech counts\n",
    "pos_counts = pd.DataFrame(num_list)\n",
    "columns = list(pos_counts.columns)\n",
    "\n",
    "# Add discipline of each paper as new column to dataframe\n",
    "idx = 0\n",
    "new_col = pos_analysis_df['DISCIPLINE']\n",
    "pos_counts.insert(loc=idx, column='DISCIPLINE', value=new_col)\n",
    "\n",
    "pos_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can calculate the amount of times, on average, that each part-of-speech appears in Biology versus English papers. To do so, you use the .groupby() and .mean() functions to group all part-of-speech counts from the Biology texts together and calculate the mean usage of each part-of-speech, before doing the same for the English texts. The following code also rounds the counts to the nearest whole number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbK1VH5lZ7T6"
   },
   "outputs": [],
   "source": [
    "# Get average part of speech counts used in papers of each discipline\n",
    "average_pos_df = pos_counts.groupby(['DISCIPLINE']).mean()\n",
    "\n",
    "# Round calculations to the nearest whole number\n",
    "average_pos_df = average_pos_df.round(0)\n",
    "\n",
    "# Reset index to improve DataFrame readability\n",
    "average_pos_df = average_pos_df.reset_index()\n",
    "\n",
    "# Show dataframe\n",
    "average_pos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can examine the differences between average part-of-speech usage per genre. As suspected, Biology student papers use slightly more adjectives (235 per paper on average) than English student papers (209 per paper on average), while an even greater number of verbs (306) are used on average in English papers than in Biology papers (237). Another interesting contrast is in the NUM tag: almost 50 more numbers are used in Biology papers, on average, than in English papers. Given the conventions of scientific research, this does makes sense; studies are much more frequently quantitative, incorporating lab measurements and statistical calculations.\n",
    "\n",
    "We can visualize these differences using a bar graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0NVs2tojM9A"
   },
   "outputs": [],
   "source": [
    "# Use plotly to plot proper noun use per genre\n",
    "fig = px.bar(average_pos_df, x=\"DISCIPLINE\", y=[\"ADJ\", 'VERB', \"NUM\"], title=\"Average Part-of-Speech Use in Papers Written by Biology and English Students\", barmode='group')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though admittedly a simple analysis, calculating part-of-speech frequency counts affirms prior studies which posit a correlation between lexico-grammatical features and disciplinary conventions, suggesting this application of spaCy can be adapted to serve other researchers’ corpora and part-of-speech usage queries9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xC6eZIpIyb8k"
   },
   "source": [
    "### Named Entity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you’ll use the named entity tags extracted from spaCy to investigate the second research question: Do students use certain named entities more frequently in different academic genres, and does this signify differences in genre conventions?\n",
    "\n",
    "To start, we’ll create a new DataFrame with the text filenames, types (genres), and named entity words and tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_WJ6sFTifpA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create new DataFrame for analysis purposes\n",
    "ner_analysis_df = final_paper_df[['Filename','PAPER TYPE', 'Named_Entities', 'NE_Words']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the str.count method, we can get counts of a specific named entity used in each text. Let’s get the counts of the named entities of interest here (PERSON, ORG, DATE, and WORKS_OF_ART) and add them as new columns of the DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MgWXgOL4K2E"
   },
   "outputs": [],
   "source": [
    "# Convert named entity lists to strings so we can count specific entities\n",
    "ner_analysis_df['Named_Entities'] = ner_analysis_df['Named_Entities'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Get the number of each type of entity in each paper\n",
    "person_counts = ner_analysis_df['Named_Entities'].str.count('PERSON')\n",
    "loc_counts = ner_analysis_df['Named_Entities'].str.count('LOC')\n",
    "date_counts = ner_analysis_df['Named_Entities'].str.count('DATE')\n",
    "woa_counts = ner_analysis_df['Named_Entities'].str.count('WORK_OF_ART')\n",
    "\n",
    "# Append named entity counts to new DataFrame\n",
    "ner_counts_df = pd.DataFrame()\n",
    "ner_counts_df['Genre'] = ner_analysis_df[\"PAPER TYPE\"]\n",
    "ner_counts_df['PERSON_Counts'] = person_counts\n",
    "ner_counts_df['LOC_Counts'] = loc_counts\n",
    "ner_counts_df['DATE_Counts'] = date_counts\n",
    "ner_counts_df['WORK_OF_ART_Counts'] = woa_counts\n",
    "\n",
    "ner_counts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can compare the average usage of each named entity and plot across paper type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXUlrCXhh_0f"
   },
   "outputs": [],
   "source": [
    "# Calculate average usage of each named entity type\n",
    "average_ner_df = ner_counts_df.groupby(['Genre']).mean()\n",
    "average_ner_df = average_ner_df.round(0)\n",
    "average_ner_df = average_ner_df.reset_index()\n",
    "average_ner_df\n",
    "\n",
    "# Use plotly to plot proper noun use per genre\n",
    "fig = px.bar(average_ner_df, x=\"Genre\", y=[\"PERSON_Counts\", 'LOC_Counts', \"DATE_Counts\", 'WORK_OF_ART_Counts'], title=\"Average Named Entity Usage Across Student Paper Genres\", barmode='group')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As hypothesized at the start of this lab: more dates and numbers are used in description-heavy proposals and research papers, while more people and works of art are referenced in arguments and critiques/evaluations. Both of these hypotheses are predicated on engaging with and assessing other scholarship.\n",
    "\n",
    "Interestingly, people and locations are used the most frequently on average across all genres, likely because these concepts often appear in citations. Overall, locations are most frequently invoked in proposals and reports. Though this should be investigated further through close reading, it does follow that these genres would use locations frequently because they are often grounded in real-world spaces in which events are being reported or imagined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8nl8w084fo4"
   },
   "source": [
    "### Analysis of ```DATE``` Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s explore patterns of one of these entities’ usage (DATE) further by retrieving the words most frequently tagged as dates in various genres. You’ll do this by first creating functions to extract the words tagged as date entities in each document and adding the words to a new DataFrame column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RvvclvkA_HM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define function to extract words tagged as \"date\" named entities from doc objects\n",
    "def extract_date_named_entities(doc):\n",
    "    return [ent for ent in doc.ents if ent.label_ == 'DATE']\n",
    "\n",
    "# Get all date entity words and apply to new column of DataFrame\n",
    "ner_analysis_df['Date_Named_Entities'] = final_paper_df['Doc'].apply(extract_date_named_entities)\n",
    "\n",
    "\n",
    "# Make list of date entities a string so we can count their frequencies\n",
    "ner_analysis_df['Date_Named_Entities'] = [', '.join(map(str, l)) for l in ner_analysis_df['Date_Named_Entities']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can retrieve only the subset of papers that are in the proposal genre, get the top words that have been tagged as “dates” in these papers and append them to a list.spaCy outputs a list of the 10 words most-frequently labeled with the DATE named entity tag in Proposal papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELHvnBtNvLSH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Search for only date words in proposal papers\n",
    "date_word_counts_df = ner_analysis_df[(ner_analysis_df == 'Proposal').any(axis=1)]\n",
    "\n",
    "# Count the frequency of each word in these essays and append to list\n",
    "date_word_frequencies = date_word_counts_df.Date_Named_Entities.str.split(expand=True).stack().value_counts()\n",
    "\n",
    "# Get top 10 most common words and their frequencies\n",
    "date_word_frequencies[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority are standard 4-digit dates; though further analysis is certainly needed to confirm, these date entities seem to indicate citation references are occurring. This fits in with our expectations of the proposal genre, which requires references to prior scholarship to justify students’ proposed claims.\n",
    "\n",
    "Let’s contrast this with the top DATE entities in Critique/Evaluation papers. Now, spaCy outputs a list of the 10 words most-frequently labeled with the DATE named entity tag in Critique/Evaluation papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iKKPP-swqe2"
   },
   "outputs": [],
   "source": [
    "# Search for only date words in critique/evaluation papers\n",
    "date_word_counts_df = ner_analysis_df[(ner_analysis_df == 'Critique/Evaluation').any(axis=1)]\n",
    "\n",
    "# Count the frequency of each word in these essays and append to list\n",
    "date_word_frequencies = date_word_counts_df.Date_Named_Entities.str.split(expand=True).stack().value_counts()\n",
    "\n",
    "# Get top 10 most common words and their frequencies\n",
    "date_word_frequencies[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, only three of the most-frequently tagged DATE entities are standard 4-digit dates, and the rest are noun references to relative dates or periods. This, too, may indicate genre conventions, such as the need to provide context and/or center an argument in relative space and time in evaluative work. Future research could analyze chains of named entities (and parts-of-speech) to get a better understanding of how these features together indicate larger rhetorical tactics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "In this lab, we’ve gleaned more information about the grammatical makeup of a text corpus. Such information can be valuable to researchers who are seeking to understand differences between texts in their corpus: What types of named entities are most common across the corpus? How frequently are certain words used as nouns versus objects within individual texts and corpora? What may these frequencies reveal about the content or themes of the texts themselves?\n",
    "\n",
    "While we’ve covered the basics of spaCy in this lab, it has other capacities, such as word vectorization and custom rule-based tagging, that are certainly worth exploring in more detail. The code can also be altered to work with custom feature sets. A great example of working with custom feaature sets is Susan Grunewald’s and Andrew Janco’s lesson, Finding Places in Text with the World Historical Gazetteer, in which spaCy is leveraged to identify place names of German prisoner of war camps in World War II memoirs, drawing on a historical gazetteer of camp names:\n",
    "https://programminghistorian.org/en/lessons/finding-places-world-historical-gazetteer#4-building-a-gazetteer\n",
    "\n",
    "spaCy is an equally helpful tool to explore texts without fully-formed research questions in mind. Exploring linguistic annotations can propel further research questions and guide the development of text-mining methods.\n",
    "\n",
    "Ultimately, this lab has provided a foundation for corpus analysis with spaCy. Whether you wish to investigate language use in student papers, novels, or another large collection of texts, this code can be repurposed for your use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
